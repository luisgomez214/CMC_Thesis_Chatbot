2025-02-25 11:44:46,795 - INFO - Loaded 3460 theses from database
2025-02-25 11:44:47,051 - INFO - TF-IDF embeddings prepared successfully
2025-02-25 11:44:53,635 - INFO - Query: bryan marin | Results: 1 | Time: 0.06s | Method: direct_match
2025-02-25 11:44:53,636 - INFO - Making new LLM API call
2025-02-25 11:44:53,977 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 11:45:41,035 - INFO - Expanded query with terms: ['paper', 'dissertation', 'research', 'study']
2025-02-25 11:45:41,132 - INFO - Found potential author match: ask
2025-02-25 11:45:41,132 - INFO - Query: i am writing a thesis about mexican health who should i ask to be my advisor | Results: 5 | Time: 0.10s | Method: author_match
2025-02-25 11:45:41,139 - INFO - Making new LLM API call
2025-02-25 11:45:41,746 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 11:45:41,759 - INFO - Making new LLM API call
2025-02-25 11:45:42,734 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 11:46:34,496 - INFO - Query: mike izbicki | Results: 15 | Time: 0.07s | Method: direct_match
2025-02-25 11:46:34,499 - INFO - Making new LLM API call
2025-02-25 11:46:34,790 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 11:46:34,801 - INFO - Using cached LLM response
2025-02-25 11:47:06,428 - INFO - Making new LLM API call
2025-02-25 11:47:06,805 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 11:47:06,810 - INFO - Using cached LLM response
2025-02-25 11:47:39,883 - INFO - Expanded query with terms: ['paper', 'dissertation', 'research', 'study']
2025-02-25 11:47:39,978 - INFO - Found keyword match on: writing
2025-02-25 11:47:39,978 - INFO - Query: i am writing a thesis about mexican health who should be my advisor | Results: 13 | Time: 0.09s | Method: keyword_match
2025-02-25 11:47:39,982 - INFO - Making new LLM API call
2025-02-25 11:47:40,290 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 11:47:40,304 - INFO - Making new LLM API call
2025-02-25 11:47:40,515 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 11:56:24,632 - INFO - Loaded 3460 theses from database
2025-02-25 11:56:24,889 - INFO - TF-IDF embeddings prepared successfully
2025-02-25 11:56:28,584 - INFO - Query: bryan marin | Results: 1 | Time: 0.06s | Method: direct_match
2025-02-25 11:56:28,587 - INFO - Using cached LLM response
2025-02-25 11:56:37,473 - INFO - Expanded query with terms: ['paper', 'dissertation', 'research', 'study', 'from', 'author', 'written']
2025-02-25 11:56:37,547 - INFO - Found potential author match: bryan
2025-02-25 11:56:37,547 - INFO - Query: thesis written by bryan marin | Results: 7 | Time: 0.07s | Method: author_match
2025-02-25 11:56:37,550 - INFO - Using cached LLM response
2025-02-25 11:56:37,551 - INFO - Making new LLM API call
2025-02-25 11:56:38,207 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 11:56:57,739 - INFO - Using cached LLM response
2025-02-25 11:56:57,740 - INFO - Using cached LLM response
2025-02-25 11:59:57,776 - INFO - Using cached LLM response
2025-02-25 11:59:57,778 - INFO - Using cached LLM response
2025-02-25 12:00:46,857 - INFO - Loaded 3460 theses from database
2025-02-25 12:00:47,117 - INFO - TF-IDF embeddings prepared successfully
2025-02-25 12:00:55,928 - INFO - Expanded query with terms: ['paper', 'dissertation', 'research', 'study', 'from', 'author', 'written']
2025-02-25 12:00:56,001 - INFO - Found potential author match: bryan
2025-02-25 12:00:56,001 - INFO - Query: thesis created by bryan marin | Results: 7 | Time: 0.07s | Method: author_match
2025-02-25 12:00:56,003 - INFO - Using cached LLM response
2025-02-25 12:00:56,004 - INFO - Using cached LLM response
2025-02-25 12:01:06,465 - INFO - Using cached LLM response
2025-02-25 12:01:06,466 - INFO - Using cached LLM response
2025-02-25 13:21:03,158 - INFO - Loaded 3460 theses from database
2025-02-25 13:21:03,418 - INFO - TF-IDF embeddings prepared successfully
2025-02-25 13:22:31,870 - INFO - Loaded 3460 theses from database
2025-02-25 13:22:32,127 - INFO - TF-IDF embeddings prepared successfully
2025-02-25 13:22:34,741 - INFO - Query: mike izbicki | Results: 15 | Time: 0.06s | Method: direct_match
2025-02-25 13:22:34,748 - INFO - Using cached LLM response
2025-02-25 13:22:34,748 - INFO - Using cached LLM response
2025-02-25 13:23:43,445 - INFO - Loaded 3460 theses from database
2025-02-25 13:23:43,712 - INFO - TF-IDF embeddings prepared successfully
2025-02-25 13:23:47,978 - INFO - Expanded query with terms: ['paper', 'dissertation', 'research', 'study']
2025-02-25 13:23:48,052 - INFO - Found keyword match on: 2020
2025-02-25 13:23:48,052 - INFO - Query: thesis from 2020 | Results: 1 | Time: 0.07s | Method: keyword_match
2025-02-25 13:23:48,054 - INFO - Making new LLM API call
2025-02-25 13:23:48,352 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 13:26:59,498 - INFO - Loaded 3460 theses from database
2025-02-25 13:26:59,756 - INFO - TF-IDF embeddings prepared successfully
2025-02-25 13:27:07,426 - INFO - Expanded query with terms: ['paper', 'dissertation', 'research', 'study']
2025-02-25 13:27:07,487 - INFO - Found potential author match: bryan
2025-02-25 13:27:07,487 - INFO - Query: bryan marin thesis | Results: 7 | Time: 0.06s | Method: author_match
2025-02-25 13:27:07,492 - INFO - Using cached LLM response
2025-02-25 13:27:07,492 - INFO - Using cached LLM response
2025-02-25 13:27:20,319 - INFO - Using cached LLM response
2025-02-25 13:27:20,320 - INFO - Using cached LLM response
2025-02-25 13:29:17,002 - INFO - Query: mike izbicki | Results: 15 | Time: 0.07s | Method: direct_match
2025-02-25 13:29:17,006 - INFO - Using cached LLM response
2025-02-25 13:29:17,007 - INFO - Using cached LLM response
2025-02-25 13:31:38,334 - INFO - Loaded 3460 theses from database
2025-02-25 13:31:38,592 - INFO - TF-IDF embeddings prepared successfully
2025-02-25 13:31:42,038 - INFO - Query: mike izbicki | Results: 15 | Time: 0.06s | Method: direct_match
2025-02-25 13:31:42,043 - INFO - Using cached LLM response
2025-02-25 13:31:42,043 - INFO - Using cached LLM response
2025-02-25 13:31:42,043 - INFO - Using cached LLM response
2025-02-25 13:31:42,043 - INFO - Using cached LLM response
2025-02-25 13:31:42,044 - INFO - Making new LLM API call
2025-02-25 13:31:42,450 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 13:31:42,461 - INFO - Making new LLM API call
2025-02-25 13:31:42,754 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 13:31:42,768 - INFO - Using cached LLM response
2025-02-25 13:31:42,769 - INFO - Making new LLM API call
2025-02-25 13:31:43,015 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 13:31:43,020 - INFO - Making new LLM API call
2025-02-25 13:31:43,266 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 13:31:43,274 - INFO - Making new LLM API call
2025-02-25 13:31:43,516 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 13:31:59,918 - INFO - Using cached LLM response
2025-02-25 13:31:59,919 - INFO - Using cached LLM response
2025-02-25 13:31:59,920 - INFO - Using cached LLM response
2025-02-25 13:31:59,920 - INFO - Using cached LLM response
2025-02-25 13:31:59,921 - INFO - Using cached LLM response
2025-02-25 13:31:59,921 - INFO - Using cached LLM response
2025-02-25 13:31:59,921 - INFO - Using cached LLM response
2025-02-25 13:31:59,922 - INFO - Using cached LLM response
2025-02-25 13:31:59,922 - INFO - Using cached LLM response
2025-02-25 13:31:59,922 - INFO - Using cached LLM response
2025-02-25 13:38:09,763 - INFO - Loaded 3460 theses from database
2025-02-25 13:38:10,021 - INFO - TF-IDF embeddings prepared successfully
2025-02-25 13:38:14,023 - INFO - Using cached LLM response
2025-02-25 13:38:42,444 - INFO - Using cached LLM response
2025-02-25 13:38:42,445 - INFO - Using cached LLM response
2025-02-25 13:38:42,445 - INFO - Making new LLM API call
2025-02-25 13:38:42,905 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 13:38:42,917 - INFO - Making new LLM API call
2025-02-25 13:38:43,161 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 13:38:43,174 - INFO - Making new LLM API call
2025-02-25 13:38:43,428 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 13:38:43,433 - INFO - Making new LLM API call
2025-02-25 13:38:43,726 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 13:38:43,737 - INFO - Making new LLM API call
2025-02-25 13:38:43,997 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 13:39:01,476 - INFO - Making new LLM API call
2025-02-25 13:39:01,839 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 13:39:01,853 - INFO - Making new LLM API call
2025-02-25 13:39:02,159 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 13:39:02,164 - INFO - Making new LLM API call
2025-02-25 13:39:02,464 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 13:39:02,471 - INFO - Making new LLM API call
2025-02-25 13:39:02,681 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 13:39:02,686 - INFO - Making new LLM API call
2025-02-25 13:39:02,920 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 13:39:02,929 - INFO - Making new LLM API call
2025-02-25 13:39:03,180 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 13:39:03,183 - INFO - Making new LLM API call
2025-02-25 13:39:03,423 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 13:42:21,466 - INFO - Loaded 3460 theses from database
2025-02-25 13:42:21,724 - INFO - TF-IDF embeddings prepared successfully
2025-02-25 13:42:28,529 - INFO - Making new LLM API call
2025-02-25 13:42:29,117 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 13:42:29,128 - INFO - Making new LLM API call
2025-02-25 13:42:29,420 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 13:42:29,435 - INFO - Making new LLM API call
2025-02-25 13:42:29,727 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 13:42:29,733 - INFO - Making new LLM API call
2025-02-25 13:42:30,034 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 13:42:30,041 - INFO - Making new LLM API call
2025-02-25 13:42:30,444 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 13:42:30,448 - INFO - Making new LLM API call
2025-02-25 13:42:30,750 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 13:42:30,758 - INFO - Making new LLM API call
2025-02-25 13:42:31,160 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 13:42:31,164 - INFO - Making new LLM API call
2025-02-25 13:42:31,392 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 13:42:31,404 - INFO - Making new LLM API call
2025-02-25 13:42:31,672 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 13:42:31,675 - INFO - Making new LLM API call
2025-02-25 13:42:31,981 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 13:43:05,051 - INFO - Making new LLM API call
2025-02-25 13:43:05,363 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 13:43:05,369 - INFO - Making new LLM API call
2025-02-25 13:43:05,774 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 13:43:05,793 - INFO - Making new LLM API call
2025-02-25 13:43:06,057 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 13:43:06,063 - INFO - Making new LLM API call
2025-02-25 13:43:06,387 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 13:43:09,529 - INFO - Using cached LLM response
2025-02-25 13:43:09,530 - INFO - Using cached LLM response
2025-02-25 13:43:09,530 - INFO - Using cached LLM response
2025-02-25 13:43:09,530 - INFO - Using cached LLM response
2025-02-25 13:43:14,006 - INFO - Using cached LLM response
2025-02-25 13:43:14,007 - INFO - Using cached LLM response
2025-02-25 13:43:14,007 - INFO - Using cached LLM response
2025-02-25 13:43:14,008 - INFO - Using cached LLM response
2025-02-25 13:43:14,008 - INFO - Using cached LLM response
2025-02-25 13:43:14,009 - INFO - Using cached LLM response
2025-02-25 13:43:14,009 - INFO - Using cached LLM response
2025-02-25 13:43:14,009 - INFO - Using cached LLM response
2025-02-25 13:43:14,010 - INFO - Using cached LLM response
2025-02-25 13:43:14,010 - INFO - Using cached LLM response
2025-02-25 14:03:33,275 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2025-02-25 14:03:33,275 - INFO - [33mPress CTRL+C to quit[0m
2025-02-25 14:03:33,294 - INFO -  * Restarting with stat
2025-02-25 14:03:35,278 - WARNING -  * Debugger is active!
2025-02-25 14:03:35,320 - INFO -  * Debugger PIN: 522-784-086
2025-02-25 14:05:38,027 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5007
2025-02-25 14:05:38,027 - INFO - [33mPress CTRL+C to quit[0m
2025-02-25 14:05:38,028 - INFO -  * Restarting with stat
2025-02-25 14:05:40,124 - WARNING -  * Debugger is active!
2025-02-25 14:05:40,135 - INFO -  * Debugger PIN: 522-784-086
2025-02-25 14:05:55,400 - INFO - 127.0.0.1 - - [25/Feb/2025 14:05:55] "GET / HTTP/1.1" 200 -
2025-02-25 14:05:55,412 - INFO - 127.0.0.1 - - [25/Feb/2025 14:05:55] "[33mGET /CMC_logo.jpeg HTTP/1.1[0m" 404 -
2025-02-25 14:05:55,466 - INFO - 127.0.0.1 - - [25/Feb/2025 14:05:55] "[33mGET /favicon.ico HTTP/1.1[0m" 404 -
2025-02-25 14:06:10,220 - INFO - 127.0.0.1 - - [25/Feb/2025 14:06:10] "[35m[1mPOST / HTTP/1.1[0m" 500 -
2025-02-25 14:06:10,253 - INFO - 127.0.0.1 - - [25/Feb/2025 14:06:10] "GET /?__debugger__=yes&cmd=resource&f=style.css HTTP/1.1" 200 -
2025-02-25 14:06:10,254 - INFO - 127.0.0.1 - - [25/Feb/2025 14:06:10] "GET /?__debugger__=yes&cmd=resource&f=debugger.js HTTP/1.1" 200 -
2025-02-25 14:06:10,282 - INFO - 127.0.0.1 - - [25/Feb/2025 14:06:10] "GET /?__debugger__=yes&cmd=resource&f=console.png&s=WFoLNWJfxNtNJKk3yGf1 HTTP/1.1" 200 -
2025-02-25 14:08:15,137 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5008
2025-02-25 14:08:15,137 - INFO - [33mPress CTRL+C to quit[0m
2025-02-25 14:08:15,138 - INFO -  * Restarting with stat
2025-02-25 14:08:17,253 - WARNING -  * Debugger is active!
2025-02-25 14:08:17,264 - INFO -  * Debugger PIN: 522-784-086
2025-02-25 14:08:25,946 - INFO - 127.0.0.1 - - [25/Feb/2025 14:08:25] "GET / HTTP/1.1" 200 -
2025-02-25 14:08:25,956 - INFO - 127.0.0.1 - - [25/Feb/2025 14:08:25] "[33mGET /CMC_logo.jpeg HTTP/1.1[0m" 404 -
2025-02-25 14:08:26,017 - INFO - 127.0.0.1 - - [25/Feb/2025 14:08:26] "[33mGET /favicon.ico HTTP/1.1[0m" 404 -
2025-02-25 14:08:29,826 - INFO - 127.0.0.1 - - [25/Feb/2025 14:08:29] "[35m[1mPOST / HTTP/1.1[0m" 500 -
2025-02-25 14:08:29,841 - INFO - 127.0.0.1 - - [25/Feb/2025 14:08:29] "GET /?__debugger__=yes&cmd=resource&f=debugger.js HTTP/1.1" 200 -
2025-02-25 14:08:29,842 - INFO - 127.0.0.1 - - [25/Feb/2025 14:08:29] "GET /?__debugger__=yes&cmd=resource&f=style.css HTTP/1.1" 200 -
2025-02-25 14:08:29,854 - INFO - 127.0.0.1 - - [25/Feb/2025 14:08:29] "GET /?__debugger__=yes&cmd=resource&f=console.png&s=puTXFozFbg7wvopjKs8M HTTP/1.1" 200 -
2025-02-25 14:19:39,772 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5008
2025-02-25 14:19:39,772 - INFO - [33mPress CTRL+C to quit[0m
2025-02-25 14:19:39,773 - INFO -  * Restarting with stat
2025-02-25 14:19:42,005 - WARNING -  * Debugger is active!
2025-02-25 14:19:42,016 - INFO -  * Debugger PIN: 522-784-086
2025-02-25 14:19:42,831 - INFO - 127.0.0.1 - - [25/Feb/2025 14:19:42] "GET / HTTP/1.1" 200 -
2025-02-25 14:19:42,841 - INFO - 127.0.0.1 - - [25/Feb/2025 14:19:42] "[33mGET /CMC_logo.jpeg HTTP/1.1[0m" 404 -
2025-02-25 14:19:47,165 - INFO - 127.0.0.1 - - [25/Feb/2025 14:19:47] "[35m[1mPOST / HTTP/1.1[0m" 500 -
2025-02-25 14:19:47,194 - INFO - 127.0.0.1 - - [25/Feb/2025 14:19:47] "[36mGET /?__debugger__=yes&cmd=resource&f=style.css HTTP/1.1[0m" 304 -
2025-02-25 14:19:47,194 - INFO - 127.0.0.1 - - [25/Feb/2025 14:19:47] "[36mGET /?__debugger__=yes&cmd=resource&f=debugger.js HTTP/1.1[0m" 304 -
2025-02-25 14:19:47,209 - INFO - 127.0.0.1 - - [25/Feb/2025 14:19:47] "GET /?__debugger__=yes&cmd=resource&f=console.png&s=ldw8W0pniZxAeIqZla3U HTTP/1.1" 200 -
2025-02-25 14:26:04,679 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5009
2025-02-25 14:26:04,679 - INFO - [33mPress CTRL+C to quit[0m
2025-02-25 14:26:04,680 - INFO -  * Restarting with stat
2025-02-25 14:26:07,031 - WARNING -  * Debugger is active!
2025-02-25 14:26:07,041 - INFO -  * Debugger PIN: 522-784-086
2025-02-25 14:26:11,749 - INFO - 127.0.0.1 - - [25/Feb/2025 14:26:11] "GET / HTTP/1.1" 200 -
2025-02-25 14:26:11,760 - INFO - 127.0.0.1 - - [25/Feb/2025 14:26:11] "[33mGET /CMC_logo.jpeg HTTP/1.1[0m" 404 -
2025-02-25 14:26:11,814 - INFO - 127.0.0.1 - - [25/Feb/2025 14:26:11] "[33mGET /favicon.ico HTTP/1.1[0m" 404 -
2025-02-25 14:26:18,531 - INFO - 127.0.0.1 - - [25/Feb/2025 14:26:18] "[35m[1mPOST / HTTP/1.1[0m" 500 -
2025-02-25 14:26:18,549 - INFO - 127.0.0.1 - - [25/Feb/2025 14:26:18] "GET /?__debugger__=yes&cmd=resource&f=style.css HTTP/1.1" 200 -
2025-02-25 14:26:18,550 - INFO - 127.0.0.1 - - [25/Feb/2025 14:26:18] "GET /?__debugger__=yes&cmd=resource&f=debugger.js HTTP/1.1" 200 -
2025-02-25 14:26:18,560 - INFO - 127.0.0.1 - - [25/Feb/2025 14:26:18] "GET /?__debugger__=yes&cmd=resource&f=console.png&s=9hBWTQWo0yA3g82ZqiCr HTTP/1.1" 200 -
2025-02-25 14:27:24,093 - INFO - Loaded 3460 theses from database
2025-02-25 14:27:24,354 - INFO - TF-IDF embeddings prepared successfully
2025-02-25 14:27:27,854 - INFO - Query: mike izbicki | Results: 15 | Time: 0.06s | Method: direct_match
2025-02-25 14:27:27,859 - INFO - Using cached LLM response
2025-02-25 14:27:27,859 - INFO - Using cached LLM response
2025-02-25 17:27:38,615 - INFO - Loaded 3460 theses from database
2025-02-25 17:27:38,874 - INFO - TF-IDF embeddings prepared successfully
2025-02-25 17:27:42,908 - INFO - Query: mike izbicki | Results: 15 | Time: 0.04s | Method: direct_match
2025-02-25 17:27:42,934 - INFO - Using cached LLM response
2025-02-25 17:27:42,934 - INFO - Using cached LLM response
2025-02-25 17:27:51,734 - INFO - Using cached LLM response
2025-02-25 17:27:51,735 - INFO - Using cached LLM response
2025-02-25 17:28:03,824 - INFO - Using cached LLM response
2025-02-25 17:28:03,826 - INFO - Using cached LLM response
2025-02-25 17:46:43,710 - INFO - Loaded 3460 theses from database
2025-02-25 17:46:43,970 - INFO - TF-IDF embeddings prepared successfully
2025-02-25 17:47:40,943 - INFO - Using cached LLM response
2025-02-25 17:47:40,943 - INFO - Using cached LLM response
2025-02-25 17:47:51,827 - INFO - Using cached LLM response
2025-02-25 17:47:51,828 - INFO - Using cached LLM response
2025-02-25 17:49:17,124 - INFO - Loaded 3460 theses from database
2025-02-25 17:49:17,380 - INFO - TF-IDF embeddings prepared successfully
2025-02-25 17:49:22,932 - INFO - Using cached LLM response
2025-02-25 17:49:22,932 - INFO - Using cached LLM response
2025-02-25 17:51:15,654 - INFO - Loaded 3460 theses from database
2025-02-25 17:51:15,911 - INFO - TF-IDF embeddings prepared successfully
2025-02-25 17:51:23,050 - INFO - Using cached LLM response
2025-02-25 17:51:23,050 - INFO - Using cached LLM response
2025-02-25 17:52:04,604 - INFO - Loaded 3460 theses from database
2025-02-25 17:52:04,863 - INFO - TF-IDF embeddings prepared successfully
2025-02-25 17:52:13,085 - INFO - Using cached LLM response
2025-02-25 17:52:13,085 - INFO - Using cached LLM response
2025-02-25 17:52:13,086 - INFO - Using cached LLM response
2025-02-25 17:52:13,086 - INFO - Using cached LLM response
2025-02-25 17:52:13,087 - INFO - Using cached LLM response
2025-02-25 17:52:13,087 - INFO - Using cached LLM response
2025-02-25 17:52:13,087 - INFO - Using cached LLM response
2025-02-25 17:52:13,087 - INFO - Using cached LLM response
2025-02-25 17:52:13,087 - INFO - Using cached LLM response
2025-02-25 17:52:13,087 - INFO - Using cached LLM response
2025-02-25 17:52:28,877 - INFO - Using cached LLM response
2025-02-25 17:52:28,879 - INFO - Using cached LLM response
2025-02-25 17:52:28,879 - INFO - Using cached LLM response
2025-02-25 17:52:28,880 - INFO - Using cached LLM response
2025-02-25 17:52:28,880 - INFO - Using cached LLM response
2025-02-25 17:52:28,880 - INFO - Using cached LLM response
2025-02-25 17:52:28,881 - INFO - Using cached LLM response
2025-02-25 17:52:28,883 - INFO - Using cached LLM response
2025-02-25 17:52:28,883 - INFO - Using cached LLM response
2025-02-25 17:52:28,883 - INFO - Using cached LLM response
2025-02-25 17:53:17,196 - INFO - Using cached LLM response
2025-02-25 17:53:17,197 - INFO - Using cached LLM response
2025-02-25 17:53:17,198 - INFO - Using cached LLM response
2025-02-25 17:53:17,198 - INFO - Using cached LLM response
2025-02-25 17:53:17,199 - INFO - Using cached LLM response
2025-02-25 17:53:17,199 - INFO - Using cached LLM response
2025-02-25 17:53:17,199 - INFO - Using cached LLM response
2025-02-25 17:53:17,200 - INFO - Using cached LLM response
2025-02-25 17:53:17,200 - INFO - Using cached LLM response
2025-02-25 17:53:17,200 - INFO - Using cached LLM response
2025-02-25 17:56:39,528 - INFO - Loaded 3460 theses from database
2025-02-25 17:56:39,808 - INFO - TF-IDF embeddings prepared successfully
2025-02-25 17:56:45,890 - INFO - Making new LLM API call
2025-02-25 17:56:46,534 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 17:56:46,543 - INFO - Making new LLM API call
2025-02-25 17:56:46,941 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 17:56:46,950 - INFO - Making new LLM API call
2025-02-25 17:56:47,202 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 17:56:47,207 - INFO - Making new LLM API call
2025-02-25 17:56:47,557 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 17:56:47,566 - INFO - Making new LLM API call
2025-02-25 17:56:47,859 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 17:56:58,286 - INFO - Making new LLM API call
2025-02-25 17:56:58,614 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 17:56:58,626 - INFO - Making new LLM API call
2025-02-25 17:56:58,891 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 17:56:58,892 - INFO - Making new LLM API call
2025-02-25 17:56:59,054 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 17:58:15,864 - INFO - Using cached LLM response
2025-02-25 17:58:15,865 - INFO - Using cached LLM response
2025-02-25 17:58:15,865 - INFO - Making new LLM API call
2025-02-25 17:58:16,186 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 17:58:16,206 - INFO - Making new LLM API call
2025-02-25 17:58:17,062 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 17:58:17,069 - INFO - Making new LLM API call
2025-02-25 17:58:17,311 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 18:03:30,645 - ERROR - Fatal error in main: 'combined_text'
2025-02-25 18:06:02,035 - INFO - Loaded 3460 theses from database
2025-02-25 18:06:02,294 - INFO - TF-IDF embeddings prepared successfully
2025-02-25 18:06:11,248 - INFO - Using cached LLM response
2025-02-25 18:06:11,248 - INFO - Using cached LLM response
2025-02-25 18:06:11,249 - INFO - Using cached LLM response
2025-02-25 18:06:27,090 - INFO - Using cached LLM response
2025-02-25 18:06:27,091 - INFO - Using cached LLM response
2025-02-25 18:06:27,092 - INFO - Using cached LLM response
2025-02-25 18:06:27,092 - INFO - Using cached LLM response
2025-02-25 18:06:27,092 - INFO - Using cached LLM response
2025-02-25 18:06:27,092 - INFO - Using cached LLM response
2025-02-25 18:06:27,093 - INFO - Using cached LLM response
2025-02-25 18:06:27,093 - INFO - Using cached LLM response
2025-02-25 18:06:27,093 - INFO - Using cached LLM response
2025-02-25 18:06:27,093 - INFO - Using cached LLM response
2025-02-25 18:06:39,465 - INFO - Using cached LLM response
2025-02-25 18:06:39,466 - INFO - Using cached LLM response
2025-02-25 18:06:39,467 - INFO - Using cached LLM response
2025-02-25 18:06:39,467 - INFO - Using cached LLM response
2025-02-25 18:06:39,467 - INFO - Using cached LLM response
2025-02-25 18:06:39,468 - INFO - Using cached LLM response
2025-02-25 18:06:39,468 - INFO - Using cached LLM response
2025-02-25 18:06:39,468 - INFO - Using cached LLM response
2025-02-25 18:06:39,469 - INFO - Using cached LLM response
2025-02-25 18:06:39,469 - INFO - Using cached LLM response
2025-02-25 18:08:17,541 - INFO - Using cached LLM response
2025-02-25 18:08:17,544 - INFO - Using cached LLM response
2025-02-25 18:08:17,544 - INFO - Using cached LLM response
2025-02-25 18:08:17,545 - INFO - Using cached LLM response
2025-02-25 18:08:17,545 - INFO - Using cached LLM response
2025-02-25 18:08:17,545 - INFO - Using cached LLM response
2025-02-25 18:08:17,545 - INFO - Using cached LLM response
2025-02-25 18:08:17,546 - INFO - Using cached LLM response
2025-02-25 18:08:17,547 - INFO - Using cached LLM response
2025-02-25 18:08:17,548 - INFO - Using cached LLM response
2025-02-25 18:08:40,982 - INFO - Using cached LLM response
2025-02-25 18:08:40,984 - INFO - Using cached LLM response
2025-02-25 18:08:40,984 - INFO - Using cached LLM response
2025-02-25 18:08:40,984 - INFO - Using cached LLM response
2025-02-25 18:08:40,985 - INFO - Using cached LLM response
2025-02-25 18:08:40,985 - INFO - Using cached LLM response
2025-02-25 18:08:40,985 - INFO - Using cached LLM response
2025-02-25 18:08:40,986 - INFO - Using cached LLM response
2025-02-25 18:08:40,986 - INFO - Using cached LLM response
2025-02-25 18:08:40,987 - INFO - Using cached LLM response
2025-02-25 18:21:24,453 - INFO - Loaded 3460 theses from database
2025-02-25 18:21:24,718 - INFO - TF-IDF embeddings prepared successfully
2025-02-25 18:30:42,478 - INFO - Loaded 3460 theses from database
2025-02-25 18:30:42,736 - INFO - TF-IDF embeddings prepared successfully
2025-02-25 18:30:56,017 - ERROR - Error processing query: generate_recommendation() missing 1 required positional argument: 'user_query'
2025-02-25 18:31:54,564 - INFO - Loaded 3460 theses from database
2025-02-25 18:31:54,820 - INFO - TF-IDF embeddings prepared successfully
2025-02-25 18:31:59,154 - ERROR - Error processing query: generate_recommendation() missing 1 required positional argument: 'query'
2025-02-25 18:39:58,606 - INFO - Loaded 3460 theses from database
2025-02-25 18:39:58,867 - INFO - TF-IDF embeddings prepared successfully
2025-02-25 18:40:02,227 - ERROR - Error processing query: generate_recommendation() missing 1 required positional argument: 'interactive_query'
2025-02-25 18:41:03,496 - INFO - Loaded 3460 theses from database
2025-02-25 18:41:03,754 - INFO - TF-IDF embeddings prepared successfully
2025-02-25 18:41:07,700 - ERROR - Error processing query: generate_recommendation() missing 1 required positional argument: 'interactive_query'
2025-02-25 19:38:01,424 - INFO - Loaded 3460 theses from database
2025-02-25 19:38:01,684 - INFO - TF-IDF embeddings prepared successfully
2025-02-25 19:38:08,554 - INFO - Using cached LLM response
2025-02-25 19:38:08,554 - INFO - Using cached LLM response
2025-02-25 19:38:08,554 - INFO - Using cached LLM response
2025-02-25 19:38:08,555 - INFO - Using cached LLM response
2025-02-25 19:38:08,555 - INFO - Using cached LLM response
2025-02-25 19:38:08,555 - INFO - Using cached LLM response
2025-02-25 19:38:08,555 - INFO - Using cached LLM response
2025-02-25 19:38:19,963 - INFO - Making new LLM API call
2025-02-25 19:38:20,560 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 19:38:20,571 - INFO - Making new LLM API call
2025-02-25 19:38:20,865 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 19:38:20,875 - INFO - Making new LLM API call
2025-02-25 19:38:21,175 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 19:38:21,181 - INFO - Making new LLM API call
2025-02-25 19:38:21,480 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 19:38:21,491 - INFO - Making new LLM API call
2025-02-25 19:38:21,788 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 19:38:21,791 - INFO - Making new LLM API call
2025-02-25 19:38:22,095 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 19:38:22,104 - INFO - Making new LLM API call
2025-02-25 19:38:22,403 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 19:38:22,409 - INFO - Making new LLM API call
2025-02-25 19:38:22,711 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 19:38:22,726 - INFO - Making new LLM API call
2025-02-25 19:38:23,121 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 19:38:23,126 - INFO - Making new LLM API call
2025-02-25 19:38:23,426 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 19:38:41,518 - INFO - Using cached LLM response
2025-02-25 19:38:41,520 - INFO - Using cached LLM response
2025-02-25 19:38:41,521 - INFO - Using cached LLM response
2025-02-25 19:38:41,521 - INFO - Using cached LLM response
2025-02-25 19:38:41,522 - INFO - Using cached LLM response
2025-02-25 19:38:41,523 - INFO - Using cached LLM response
2025-02-25 19:38:41,524 - INFO - Using cached LLM response
2025-02-25 19:38:41,524 - INFO - Using cached LLM response
2025-02-25 19:38:41,525 - INFO - Using cached LLM response
2025-02-25 19:38:41,525 - INFO - Using cached LLM response
2025-02-25 19:40:36,851 - INFO - Loaded 3460 theses from database
2025-02-25 19:40:37,109 - INFO - TF-IDF embeddings prepared successfully
2025-02-25 19:40:41,459 - INFO - Making new LLM API call
2025-02-25 19:40:41,874 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 19:40:41,885 - INFO - Making new LLM API call
2025-02-25 19:40:42,179 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 19:40:42,195 - INFO - Making new LLM API call
2025-02-25 19:40:42,487 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 19:40:42,493 - INFO - Making new LLM API call
2025-02-25 19:40:42,793 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 19:40:42,806 - INFO - Using cached LLM response
2025-02-25 19:40:42,807 - INFO - Using cached LLM response
2025-02-25 19:40:42,807 - INFO - Making new LLM API call
2025-02-25 19:40:43,100 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 19:40:43,104 - INFO - Making new LLM API call
2025-02-25 19:40:43,405 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 19:40:43,413 - INFO - Making new LLM API call
2025-02-25 19:40:43,714 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 19:40:43,720 - INFO - Making new LLM API call
2025-02-25 19:40:44,018 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 19:41:31,569 - INFO - Using cached LLM response
2025-02-25 19:41:31,572 - INFO - Using cached LLM response
2025-02-25 19:41:31,573 - INFO - Using cached LLM response
2025-02-25 19:41:31,573 - INFO - Using cached LLM response
2025-02-25 19:41:31,574 - INFO - Using cached LLM response
2025-02-25 19:41:31,574 - INFO - Using cached LLM response
2025-02-25 19:41:31,575 - INFO - Using cached LLM response
2025-02-25 19:41:31,575 - INFO - Using cached LLM response
2025-02-25 19:41:31,575 - INFO - Using cached LLM response
2025-02-25 19:41:31,576 - INFO - Using cached LLM response
2025-02-25 19:42:05,500 - INFO - Using cached LLM response
2025-02-25 19:42:05,502 - INFO - Using cached LLM response
2025-02-25 19:42:05,503 - INFO - Using cached LLM response
2025-02-25 19:42:05,503 - INFO - Using cached LLM response
2025-02-25 19:42:05,504 - INFO - Using cached LLM response
2025-02-25 19:42:05,504 - INFO - Using cached LLM response
2025-02-25 19:42:05,504 - INFO - Using cached LLM response
2025-02-25 19:42:05,504 - INFO - Using cached LLM response
2025-02-25 19:42:05,505 - INFO - Using cached LLM response
2025-02-25 19:42:05,505 - INFO - Using cached LLM response
2025-02-25 19:43:54,055 - INFO - Loaded 3460 theses from database
2025-02-25 19:43:54,315 - INFO - TF-IDF embeddings prepared successfully
2025-02-25 19:45:37,922 - INFO - Loaded 3460 theses from database
2025-02-25 19:45:38,186 - INFO - TF-IDF embeddings prepared successfully
2025-02-25 19:45:40,904 - INFO - Making new LLM API call
2025-02-25 19:45:41,289 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 19:45:55,155 - INFO - Making new LLM API call
2025-02-25 19:45:55,622 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 19:45:55,638 - INFO - Making new LLM API call
2025-02-25 19:45:55,929 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 19:45:55,936 - INFO - Making new LLM API call
2025-02-25 19:45:56,236 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 19:45:56,249 - INFO - Making new LLM API call
2025-02-25 19:45:56,508 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 19:45:56,514 - INFO - Making new LLM API call
2025-02-25 19:45:56,853 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 19:45:56,869 - INFO - Making new LLM API call
2025-02-25 19:45:57,157 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 19:45:57,164 - INFO - Making new LLM API call
2025-02-25 19:45:57,422 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 19:45:57,435 - INFO - Using cached LLM response
2025-02-25 19:45:57,436 - INFO - Making new LLM API call
2025-02-25 19:45:57,771 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 19:45:57,776 - INFO - Making new LLM API call
2025-02-25 19:45:58,077 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 19:47:10,193 - INFO - Making new LLM API call
2025-02-25 19:47:10,470 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 19:47:24,876 - INFO - Using cached LLM response
2025-02-25 19:47:24,877 - INFO - Using cached LLM response
2025-02-25 19:47:24,877 - INFO - Using cached LLM response
2025-02-25 19:47:24,877 - INFO - Using cached LLM response
2025-02-25 19:47:24,877 - INFO - Using cached LLM response
2025-02-25 19:47:56,784 - INFO - Using cached LLM response
2025-02-25 19:47:56,785 - INFO - Using cached LLM response
2025-02-25 19:47:56,786 - INFO - Using cached LLM response
2025-02-25 19:47:56,786 - INFO - Using cached LLM response
2025-02-25 19:47:56,786 - INFO - Using cached LLM response
2025-02-25 19:47:56,786 - INFO - Using cached LLM response
2025-02-25 19:47:56,786 - INFO - Using cached LLM response
2025-02-25 19:47:56,786 - INFO - Using cached LLM response
2025-02-25 19:47:56,786 - INFO - Using cached LLM response
2025-02-25 19:47:56,787 - INFO - Using cached LLM response
2025-02-25 19:48:14,029 - INFO - Using cached LLM response
2025-02-25 19:48:14,029 - INFO - Using cached LLM response
2025-02-25 19:48:14,029 - INFO - Using cached LLM response
2025-02-25 19:48:14,029 - INFO - Using cached LLM response
2025-02-25 19:48:14,029 - INFO - Using cached LLM response
2025-02-25 19:48:14,029 - INFO - Using cached LLM response
2025-02-25 19:48:14,030 - INFO - Using cached LLM response
2025-02-25 19:48:14,030 - INFO - Using cached LLM response
2025-02-25 19:48:14,030 - INFO - Using cached LLM response
2025-02-25 19:48:14,030 - INFO - Using cached LLM response
2025-02-25 19:48:46,365 - INFO - Making new LLM API call
2025-02-25 19:48:46,731 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 19:48:46,758 - INFO - Making new LLM API call
2025-02-25 19:48:47,037 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 19:48:47,044 - INFO - Making new LLM API call
2025-02-25 19:48:47,345 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 19:48:47,372 - INFO - Making new LLM API call
2025-02-25 19:48:47,605 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 19:48:47,612 - INFO - Making new LLM API call
2025-02-25 19:48:47,857 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 19:48:47,874 - INFO - Making new LLM API call
2025-02-25 19:48:48,165 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 19:48:48,172 - INFO - Making new LLM API call
2025-02-25 19:48:48,572 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 19:48:48,589 - INFO - Making new LLM API call
2025-02-25 19:48:48,882 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 19:48:48,889 - INFO - Making new LLM API call
2025-02-25 19:48:49,290 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 19:48:49,306 - INFO - Making new LLM API call
2025-02-25 19:48:49,596 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 19:49:18,616 - INFO - Loaded 3460 theses from database
2025-02-25 19:49:18,873 - INFO - TF-IDF embeddings prepared successfully
2025-02-25 19:49:48,944 - INFO - Using cached LLM response
2025-02-25 19:49:48,946 - INFO - Using cached LLM response
2025-02-25 19:49:48,946 - INFO - Using cached LLM response
2025-02-25 19:49:48,946 - INFO - Using cached LLM response
2025-02-25 19:49:48,946 - INFO - Using cached LLM response
2025-02-25 20:04:58,418 - INFO - Loaded 3460 theses from database
2025-02-25 20:04:58,682 - INFO - TF-IDF embeddings prepared successfully
2025-02-25 20:05:03,095 - INFO - Using cached LLM response
2025-02-25 22:27:04,096 - INFO - Loaded 3460 theses from database
2025-02-25 22:27:04,356 - INFO - TF-IDF embeddings prepared successfully
2025-02-25 22:27:26,032 - INFO - Making new LLM API call
2025-02-25 22:27:26,540 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 22:27:26,553 - INFO - Making new LLM API call
2025-02-25 22:27:26,963 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 22:27:26,986 - INFO - Making new LLM API call
2025-02-25 22:27:27,267 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 22:27:27,275 - INFO - Making new LLM API call
2025-02-25 22:27:27,575 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 22:27:27,591 - INFO - Making new LLM API call
2025-02-25 22:27:27,831 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 22:27:27,839 - INFO - Making new LLM API call
2025-02-25 22:27:28,189 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 22:27:28,208 - INFO - Making new LLM API call
2025-02-25 22:27:28,497 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 22:27:28,504 - INFO - Making new LLM API call
2025-02-25 22:27:28,765 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 22:27:28,781 - INFO - Making new LLM API call
2025-02-25 22:27:29,034 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 22:27:29,041 - INFO - Making new LLM API call
2025-02-25 22:27:30,648 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 22:28:06,358 - INFO - Using cached LLM response
2025-02-25 22:28:06,360 - INFO - Using cached LLM response
2025-02-25 22:28:06,361 - INFO - Using cached LLM response
2025-02-25 22:28:06,361 - INFO - Using cached LLM response
2025-02-25 22:28:06,362 - INFO - Using cached LLM response
2025-02-25 22:28:06,362 - INFO - Using cached LLM response
2025-02-25 22:28:06,362 - INFO - Using cached LLM response
2025-02-25 22:28:06,362 - INFO - Using cached LLM response
2025-02-25 22:28:06,363 - INFO - Using cached LLM response
2025-02-25 22:28:06,363 - INFO - Using cached LLM response
2025-02-25 22:29:22,354 - INFO - Using cached LLM response
2025-02-25 22:29:22,356 - INFO - Using cached LLM response
2025-02-25 22:29:22,356 - INFO - Using cached LLM response
2025-02-25 22:29:22,356 - INFO - Using cached LLM response
2025-02-25 22:29:22,357 - INFO - Using cached LLM response
2025-02-25 22:29:22,357 - INFO - Using cached LLM response
2025-02-25 22:29:22,357 - INFO - Using cached LLM response
2025-02-25 22:29:22,357 - INFO - Using cached LLM response
2025-02-25 22:29:22,357 - INFO - Using cached LLM response
2025-02-25 22:29:22,357 - INFO - Using cached LLM response
2025-02-25 22:29:42,521 - INFO - Loaded 3460 theses from database
2025-02-25 22:29:42,782 - INFO - TF-IDF embeddings prepared successfully
2025-02-25 22:30:54,793 - INFO - Loaded 3460 theses from database
2025-02-25 22:30:55,051 - INFO - TF-IDF embeddings prepared successfully
2025-02-25 22:30:59,471 - INFO - Using cached LLM response
2025-02-25 22:31:16,909 - INFO - Using cached LLM response
2025-02-25 22:45:30,394 - INFO - Making new LLM API call
2025-02-25 22:45:31,067 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 22:46:34,536 - INFO - Loaded 3460 theses from database
2025-02-25 22:46:34,844 - INFO - TF-IDF embeddings prepared successfully
2025-02-25 22:46:39,839 - INFO - Using cached LLM response
2025-02-25 22:47:24,809 - INFO - Loaded 3460 theses from database
2025-02-25 22:47:25,066 - INFO - TF-IDF embeddings prepared successfully
2025-02-25 22:47:28,773 - ERROR - Error processing query: name 'retrieve_relevant_theses' is not defined
2025-02-25 22:48:48,997 - INFO - Loaded 3460 theses from database
2025-02-25 22:48:49,259 - INFO - TF-IDF embeddings prepared successfully
2025-02-25 22:48:52,656 - ERROR - Error processing query: name 'call_llm' is not defined
2025-02-25 22:51:42,377 - INFO - Loaded 3460 theses from database
2025-02-25 22:51:42,650 - INFO - TF-IDF embeddings prepared successfully
2025-02-25 22:51:48,907 - ERROR - Error processing query: name 'retrieve_relevant_theses' is not defined
2025-02-25 22:52:39,171 - INFO - Loaded 3460 theses from database
2025-02-25 22:52:39,428 - INFO - TF-IDF embeddings prepared successfully
2025-02-25 22:52:44,856 - ERROR - Error processing query: name 'retrieve_relevant_theses' is not defined
2025-02-25 23:06:03,034 - INFO - Loaded 3460 theses from database
2025-02-25 23:06:03,295 - INFO - TF-IDF embeddings prepared successfully
2025-02-25 23:06:06,528 - ERROR - Error processing query: name 'retrieve_relevant_theses' is not defined
2025-02-25 23:19:26,257 - INFO - Loaded 3460 theses from database
2025-02-25 23:19:26,517 - INFO - TF-IDF embeddings prepared successfully
2025-02-25 23:19:30,822 - INFO - Using LLM-powered search for better results
2025-02-25 23:19:30,827 - INFO - Making new LLM API call
2025-02-25 23:19:31,724 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 23:19:31,736 - ERROR - Error parsing LLM response: Expecting value: line 1 column 1 (char 0)
2025-02-25 23:19:31,736 - ERROR - LLM response was: After evaluating the candidate theses, I found the following 5 most relevant theses to the query "bryan marin". Here are the results in JSON format:

```
{
  "relevant_theses": [
    {"id": "None", "relevance_score": 0.0, "reason": "No relevant theses found"},
  ]
}
```

Unfortunately, none of the candidate theses match the query "bryan marin". The query mentions a specific author, "Bryan Marin", but none of the theses in the candidate list have this author. The theses also do not contain the query keywords.
2025-02-25 23:19:31,736 - INFO - Falling back to traditional search due to LLM parsing failure
2025-02-25 23:19:31,738 - INFO - Using cached LLM response
2025-02-25 23:19:42,986 - INFO - Using traditional search results (sufficient matches found)
2025-02-25 23:19:42,987 - INFO - Using cached LLM response
2025-02-25 23:19:42,987 - INFO - Using cached LLM response
2025-02-25 23:19:42,987 - INFO - Using cached LLM response
2025-02-25 23:19:42,987 - INFO - Using cached LLM response
2025-02-25 23:19:42,987 - INFO - Using cached LLM response
2025-02-25 23:19:42,988 - INFO - Using cached LLM response
2025-02-25 23:19:42,988 - INFO - Using cached LLM response
2025-02-25 23:19:42,988 - INFO - Using cached LLM response
2025-02-25 23:19:42,988 - INFO - Using cached LLM response
2025-02-25 23:19:42,988 - INFO - Using cached LLM response
2025-02-25 23:34:35,049 - INFO - Loaded 3460 theses from database
2025-02-25 23:34:35,319 - INFO - TF-IDF embeddings prepared successfully
2025-02-25 23:34:38,804 - INFO - Using keyword-based search results
2025-02-25 23:34:38,809 - INFO - Making new LLM API call
2025-02-25 23:34:39,317 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 23:34:39,337 - INFO - Making new LLM API call
2025-02-25 23:34:39,600 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 23:34:39,615 - INFO - Making new LLM API call
2025-02-25 23:34:39,915 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 23:34:39,919 - INFO - Making new LLM API call
2025-02-25 23:34:40,222 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 23:34:40,237 - INFO - Making new LLM API call
2025-02-25 23:34:40,531 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 23:37:23,065 - INFO - Loaded 3460 theses from database
2025-02-25 23:37:23,324 - INFO - TF-IDF embeddings prepared successfully
2025-02-25 23:37:25,772 - ERROR - Error processing query: name 'retrieve_relevant_theses' is not defined
2025-02-25 23:39:15,187 - INFO - Loaded 3460 theses from database
2025-02-25 23:39:15,444 - INFO - TF-IDF embeddings prepared successfully
2025-02-25 23:39:19,252 - INFO - Using cached LLM response
2025-02-25 23:39:19,252 - ERROR - Error parsing LLM response: Expecting value: line 1 column 1 (char 0)
2025-02-25 23:39:19,252 - ERROR - LLM response was: After evaluating the candidate theses, I found the following 5 most relevant theses to the query "bryan marin". Here are the results in JSON format:

```
{
  "relevant_theses": [
    {"id": "None", "relevance_score": 0.0, "reason": "No relevant theses found"},
  ]
}
```

Unfortunately, none of the candidate theses match the query "bryan marin". The query mentions a specific author, "Bryan Marin", but none of the theses in the candidate list have this author. The theses also do not contain the query keywords.
2025-02-25 23:39:19,252 - INFO - Falling back to traditional search due to LLM parsing failure
2025-02-25 23:39:19,252 - ERROR - Error processing query: name 'traditional_results' is not defined
2025-02-25 23:39:59,074 - INFO - Loaded 3460 theses from database
2025-02-25 23:39:59,339 - INFO - TF-IDF embeddings prepared successfully
2025-02-25 23:40:03,525 - INFO - Using cached LLM response
2025-02-25 23:40:03,525 - ERROR - Error parsing LLM response: Expecting value: line 1 column 1 (char 0)
2025-02-25 23:40:03,525 - ERROR - LLM response was: After evaluating the candidate theses, I found the following 5 most relevant theses to the query "bryan marin". Here are the results in JSON format:

```
{
  "relevant_theses": [
    {"id": "None", "relevance_score": 0.0, "reason": "No relevant theses found"},
  ]
}
```

Unfortunately, none of the candidate theses match the query "bryan marin". The query mentions a specific author, "Bryan Marin", but none of the theses in the candidate list have this author. The theses also do not contain the query keywords.
2025-02-25 23:40:03,525 - ERROR - Error processing query: 'NoneType' object has no attribute 'empty'
2025-02-25 23:42:49,853 - INFO - Loaded 3460 theses from database
2025-02-25 23:42:50,115 - INFO - TF-IDF embeddings prepared successfully
2025-02-25 23:42:53,105 - INFO - Using cached LLM response
2025-02-25 23:42:53,106 - ERROR - Error parsing LLM response: Expecting value: line 1 column 1 (char 0)
2025-02-25 23:42:53,106 - ERROR - LLM response was: After evaluating the candidate theses, I found the following 5 most relevant theses to the query "bryan marin". Here are the results in JSON format:

```
{
  "relevant_theses": [
    {"id": "None", "relevance_score": 0.0, "reason": "No relevant theses found"},
  ]
}
```

Unfortunately, none of the candidate theses match the query "bryan marin". The query mentions a specific author, "Bryan Marin", but none of the theses in the candidate list have this author. The theses also do not contain the query keywords.
2025-02-25 23:42:53,106 - INFO - Query: bryan marin | Results: 0 | Time: 0.02s | Method: llm
2025-02-25 23:43:14,798 - INFO - Making new LLM API call
2025-02-25 23:43:15,610 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 23:43:15,622 - ERROR - Error parsing LLM response: Expecting value: line 1 column 1 (char 0)
2025-02-25 23:43:15,622 - ERROR - LLM response was: After evaluating the relevance of each thesis to the query "Bryan Marin", I found the following 5 most relevant theses:

```
{
  "relevant_theses": [
    {"id": 1148, "relevance_score": 0.9, "reason": "The thesis title includes the author's name 'warren wood' and the keywords 'Bryan' is not present"},
    {"id": 1159, "relevance_score": 0.8, "reason": "The thesis title and abstract do not match the query, but the author's name 'jamie curran' is not present, and the keywords are not relevant"},
    {"id": 1154, "relevance_score": 0.7, "reason": "The thesis title and abstract do not match the query, and the author's name 'sophia a kobus-pigg' is not present, but the keywords are not relevant"},
    {"id": 1156, "relevance_score": 0.6, "reason": "The thesis title and abstract do not match the query, and the author's name 'diana m ciuca' is not present, but the keywords are not relevant"},
    {"id": 1142, "relevance_score": 0.5, "reason": "The thesis title and abstract do not match the query, and the author's name 'joseph dorn' is not present, but the keywords are not relevant"}
  ]
}
```

Note that none of the theses in the candidate list match the query "Bryan Marin" exactly, so the relevance scores are relatively low.
2025-02-25 23:43:15,624 - INFO - Query: Bryan Marin | Results: 0 | Time: 0.84s | Method: llm
2025-02-25 23:43:19,149 - INFO - Making new LLM API call
2025-02-25 23:43:19,666 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 23:43:19,685 - ERROR - Error parsing LLM response: Expecting value: line 1 column 1 (char 0)
2025-02-25 23:43:19,686 - ERROR - LLM response was: After evaluating the relevance of each thesis based on the query "bryan marin", I found the top 5 most relevant theses. Here are the results in JSON format:

```
{
  "relevant_theses": []
}
```

Unfortunately, none of the theses in the candidate list match the query "bryan marin". The query does not contain any keywords or phrases that match the title, abstract, or author names of any of the theses. Therefore, I cannot return any relevant theses.
2025-02-25 23:43:19,687 - INFO - Query: bryan  marin | Results: 0 | Time: 0.55s | Method: llm
2025-02-25 23:43:24,391 - INFO - Making new LLM API call
2025-02-25 23:43:24,453 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-02-25 23:43:24,455 - INFO - Retrying request to /openai/v1/chat/completions in 19.000000 seconds
2025-02-25 23:43:44,281 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 23:43:44,288 - ERROR - Error parsing LLM response: Expecting value: line 1 column 1 (char 0)
2025-02-25 23:43:44,288 - ERROR - LLM response was: After analyzing the candidate theses, I found that none of them exactly match the query "mike izbicki". However, I can provide a list of the most relevant theses based on the factors mentioned in the instructions.

Here are the results in JSON format:

```
{
  "relevant_theses": [
    {"id": "3335", "relevance_score": 0.6, "reason": "The thesis title 'AI in Scriptwriting: Can a Computer Capture Human Emotion?' is related to AI and human emotion, which might be relevant to the query"},
    {"id": "2475", "relevance_score": 0.5, "reason": "The thesis title 'Using Twitter Data to Forecast the Results of the 2020 Presidential Election' is related to Twitter data and election forecasting, which might be relevant to the query"},
    {"id": "2993", "relevance_score": 0.4, "reason": "The thesis title 'SentiWordNet and VADER: Comparative Analysis of the Efficacy of Hybrid Sentiment Analysis Models' is related to sentiment analysis and natural language processing, which might be relevant to the query"},
    {"id": "2211", "relevance_score": 0.3, "reason": "The thesis title 'A Little Birdy Told Me: Analysis of the Impact of Public Tweet Sentiment on Stock Prices' is related to tweet sentiment and stock prices, which might be relevant to the query"},
    {"id": "3035", "relevance_score": 0.2, "reason": "The thesis title 'Analyzing Trends in the Global Electric Vehicle Conversation on Twitter' is related to Twitter data and electric vehicles, which might be relevant to the query"}
  ]
}
```

Note that the relevance scores are subjective and based on my analysis of the thesis titles, keywords, and abstracts. The actual relevance of a thesis to the query may vary depending on the specific context and requirements.
2025-02-25 23:43:44,289 - INFO - Query: mike izbicki | Results: 0 | Time: 19.91s | Method: llm
2025-02-25 23:48:46,941 - INFO - Loaded 3460 theses from database
2025-02-25 23:48:47,204 - INFO - TF-IDF embeddings prepared successfully
2025-02-25 23:48:51,106 - INFO - Making new LLM API call
2025-02-25 23:48:51,603 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 23:48:51,616 - INFO - Using cached LLM response
2025-02-25 23:48:51,617 - INFO - Making new LLM API call
2025-02-25 23:48:51,907 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 23:48:51,928 - INFO - Using cached LLM response
2025-02-25 23:48:51,929 - INFO - Using cached LLM response
2025-02-25 23:49:12,859 - INFO - Making new LLM API call
2025-02-25 23:49:14,231 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 23:49:14,240 - INFO - Making new LLM API call
2025-02-25 23:49:14,810 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 23:49:14,830 - INFO - Making new LLM API call
2025-02-25 23:49:15,615 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 23:49:15,623 - INFO - Making new LLM API call
2025-02-25 23:49:16,176 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-25 23:49:16,194 - INFO - Making new LLM API call
2025-02-25 23:49:16,483 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 00:17:36,184 - INFO - Loaded 3460 theses from database
2025-02-26 00:17:36,445 - INFO - TF-IDF embeddings prepared successfully
2025-02-26 00:17:40,614 - INFO - Using cached LLM response
2025-02-26 00:17:40,614 - INFO - Using cached LLM response
2025-02-26 00:17:40,615 - INFO - Using cached LLM response
2025-02-26 00:17:40,615 - INFO - Using cached LLM response
2025-02-26 00:17:40,615 - INFO - Using cached LLM response
2025-02-26 00:17:40,615 - INFO - Using cached LLM response
2025-02-26 00:17:40,615 - INFO - Using cached LLM response
2025-02-26 00:17:40,615 - INFO - Using cached LLM response
2025-02-26 00:17:40,616 - INFO - Using cached LLM response
2025-02-26 00:17:40,616 - INFO - Using cached LLM response
2025-02-26 00:17:48,969 - INFO - Using cached LLM response
2025-02-26 00:17:48,970 - INFO - Using cached LLM response
2025-02-26 00:17:48,971 - INFO - Using cached LLM response
2025-02-26 00:17:48,971 - INFO - Using cached LLM response
2025-02-26 00:17:48,971 - INFO - Using cached LLM response
2025-02-26 00:17:48,971 - INFO - Using cached LLM response
2025-02-26 00:17:48,972 - INFO - Using cached LLM response
2025-02-26 00:17:48,972 - INFO - Using cached LLM response
2025-02-26 00:17:48,972 - INFO - Using cached LLM response
2025-02-26 00:17:48,973 - INFO - Using cached LLM response
2025-02-26 00:18:03,866 - INFO - Using cached LLM response
2025-02-26 00:18:03,868 - INFO - Using cached LLM response
2025-02-26 00:18:03,868 - INFO - Using cached LLM response
2025-02-26 00:18:03,868 - INFO - Using cached LLM response
2025-02-26 00:18:03,869 - INFO - Using cached LLM response
2025-02-26 00:18:03,869 - INFO - Using cached LLM response
2025-02-26 00:18:03,869 - INFO - Using cached LLM response
2025-02-26 00:18:03,870 - INFO - Using cached LLM response
2025-02-26 00:18:03,871 - INFO - Using cached LLM response
2025-02-26 00:18:03,871 - INFO - Using cached LLM response
2025-02-26 00:18:18,979 - INFO - Using cached LLM response
2025-02-26 00:20:04,594 - INFO - Loaded 3460 theses from database
2025-02-26 00:20:04,854 - INFO - TF-IDF embeddings prepared successfully
2025-02-26 00:20:07,697 - ERROR - Error processing query: name 'retrieve_relevant_theses' is not defined
2025-02-26 00:21:03,971 - INFO - Loaded 3460 theses from database
2025-02-26 00:21:04,230 - INFO - TF-IDF embeddings prepared successfully
2025-02-26 00:21:11,718 - INFO - Using cached LLM response
2025-02-26 00:21:11,718 - INFO - Using cached LLM response
2025-02-26 00:21:11,718 - INFO - Using cached LLM response
2025-02-26 00:21:11,718 - INFO - Using cached LLM response
2025-02-26 00:21:11,719 - INFO - Using cached LLM response
2025-02-26 00:21:11,719 - INFO - Using cached LLM response
2025-02-26 00:21:11,719 - INFO - Using cached LLM response
2025-02-26 00:21:11,719 - INFO - Using cached LLM response
2025-02-26 00:21:11,719 - INFO - Using cached LLM response
2025-02-26 00:21:11,719 - INFO - Using cached LLM response
2025-02-26 00:24:45,030 - INFO - Loaded 3460 theses from database
2025-02-26 00:24:45,290 - INFO - TF-IDF embeddings prepared successfully
2025-02-26 00:24:48,900 - INFO - Making new LLM API call
2025-02-26 00:24:49,598 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-26 00:24:49,600 - ERROR - Error calling LLM API: Error code: 413 - {'error': {'message': 'Request too large for model `llama3-8b-8192` in organization `org_01j66m4gmpfahvz5snsjhe79cb` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 371232, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-26 00:24:49,606 - ERROR - Error parsing LLM response: Expecting value: line 1 column 1 (char 0)
2025-02-26 00:24:55,223 - INFO - Making new LLM API call
2025-02-26 00:24:55,590 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-26 00:24:55,591 - ERROR - Error calling LLM API: Error code: 413 - {'error': {'message': 'Request too large for model `llama3-8b-8192` in organization `org_01j66m4gmpfahvz5snsjhe79cb` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 371232, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-26 00:24:55,608 - ERROR - Error parsing LLM response: Expecting value: line 1 column 1 (char 0)
2025-02-26 00:29:46,484 - INFO - Loaded 3460 theses from database
2025-02-26 00:29:46,745 - INFO - TF-IDF embeddings prepared successfully
2025-02-26 00:29:56,218 - INFO - Making new LLM API call
2025-02-26 00:29:57,112 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 00:29:57,128 - INFO - Using cached LLM response
2025-02-26 00:30:20,680 - INFO - Making new LLM API call
2025-02-26 00:30:21,471 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 00:30:21,488 - INFO - Using cached LLM response
2025-02-26 00:30:21,488 - INFO - Using cached LLM response
2025-02-26 00:30:21,489 - INFO - Using cached LLM response
2025-02-26 00:30:21,489 - INFO - Using cached LLM response
2025-02-26 00:30:21,489 - INFO - Using cached LLM response
2025-02-26 00:30:21,490 - INFO - Using cached LLM response
2025-02-26 00:30:21,490 - INFO - Using cached LLM response
2025-02-26 00:30:21,490 - INFO - Using cached LLM response
2025-02-26 00:30:21,491 - INFO - Using cached LLM response
2025-02-26 00:30:21,491 - INFO - Using cached LLM response
2025-02-26 00:30:59,254 - INFO - Using cached LLM response
2025-02-26 00:30:59,256 - INFO - Using cached LLM response
2025-02-26 00:30:59,257 - INFO - Using cached LLM response
2025-02-26 00:30:59,257 - INFO - Using cached LLM response
2025-02-26 00:30:59,258 - INFO - Using cached LLM response
2025-02-26 00:30:59,258 - INFO - Using cached LLM response
2025-02-26 00:30:59,258 - INFO - Using cached LLM response
2025-02-26 00:30:59,259 - INFO - Using cached LLM response
2025-02-26 00:30:59,259 - INFO - Using cached LLM response
2025-02-26 00:30:59,259 - INFO - Using cached LLM response
2025-02-26 00:31:23,316 - INFO - Making new LLM API call
2025-02-26 00:31:23,842 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 00:31:23,852 - INFO - Using cached LLM response
2025-02-26 00:31:23,853 - INFO - Using cached LLM response
2025-02-26 00:31:23,853 - INFO - Using cached LLM response
2025-02-26 00:31:23,853 - INFO - Using cached LLM response
2025-02-26 00:31:23,854 - INFO - Using cached LLM response
2025-02-26 00:31:23,854 - INFO - Using cached LLM response
2025-02-26 00:31:23,854 - INFO - Using cached LLM response
2025-02-26 00:31:23,854 - INFO - Using cached LLM response
2025-02-26 00:31:23,855 - INFO - Using cached LLM response
2025-02-26 00:31:23,855 - INFO - Using cached LLM response
2025-02-26 00:36:15,395 - INFO - Loaded 3460 theses from database
2025-02-26 00:36:15,653 - INFO - TF-IDF embeddings prepared successfully
2025-02-26 00:36:20,534 - INFO - Using cached LLM response
2025-02-26 00:36:20,535 - INFO - Using cached LLM response
2025-02-26 00:36:42,397 - INFO - Making new LLM API call
2025-02-26 00:36:44,153 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 00:36:44,166 - INFO - Making new LLM API call
2025-02-26 00:36:45,072 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 00:36:45,099 - INFO - Making new LLM API call
2025-02-26 00:36:45,584 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 00:36:45,590 - INFO - Making new LLM API call
2025-02-26 00:36:45,993 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 00:36:46,014 - INFO - Making new LLM API call
2025-02-26 00:36:46,301 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 00:36:46,311 - INFO - Making new LLM API call
2025-02-26 00:36:46,585 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 00:36:46,604 - INFO - Making new LLM API call
2025-02-26 00:36:46,914 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 00:36:46,921 - INFO - Making new LLM API call
2025-02-26 00:36:47,221 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 00:36:47,234 - INFO - Making new LLM API call
2025-02-26 00:36:47,528 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 00:39:46,599 - INFO - Making new LLM API call
2025-02-26 00:39:47,140 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 00:39:47,163 - INFO - Using cached LLM response
2025-02-26 00:39:47,163 - INFO - Using cached LLM response
2025-02-26 00:39:47,164 - INFO - Using cached LLM response
2025-02-26 00:39:47,164 - INFO - Using cached LLM response
2025-02-26 00:39:47,164 - INFO - Using cached LLM response
2025-02-26 00:40:49,770 - INFO - Using cached LLM response
2025-02-26 00:40:49,771 - INFO - Using cached LLM response
2025-02-26 00:40:49,771 - INFO - Using cached LLM response
2025-02-26 00:40:49,771 - INFO - Using cached LLM response
2025-02-26 00:40:49,772 - INFO - Using cached LLM response
2025-02-26 00:40:49,772 - INFO - Using cached LLM response
2025-02-26 00:40:49,772 - INFO - Using cached LLM response
2025-02-26 00:40:49,772 - INFO - Using cached LLM response
2025-02-26 00:40:49,772 - INFO - Using cached LLM response
2025-02-26 00:40:49,772 - INFO - Using cached LLM response
2025-02-26 00:40:49,772 - INFO - Using cached LLM response
2025-02-26 00:41:25,165 - INFO - Making new LLM API call
2025-02-26 00:41:25,716 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 00:41:25,725 - INFO - Making new LLM API call
2025-02-26 00:41:26,058 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 00:41:26,072 - INFO - Making new LLM API call
2025-02-26 00:41:26,363 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 00:41:26,369 - INFO - Making new LLM API call
2025-02-26 00:41:26,775 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 00:41:26,786 - INFO - Making new LLM API call
2025-02-26 00:41:27,082 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 00:41:27,089 - INFO - Making new LLM API call
2025-02-26 00:41:27,595 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 00:41:27,603 - INFO - Making new LLM API call
2025-02-26 00:41:27,904 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 00:41:27,911 - INFO - Making new LLM API call
2025-02-26 00:41:28,413 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 00:41:28,424 - INFO - Making new LLM API call
2025-02-26 00:41:28,928 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 00:41:28,934 - INFO - Making new LLM API call
2025-02-26 00:41:29,439 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 00:41:29,449 - INFO - Making new LLM API call
2025-02-26 00:41:29,744 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 00:46:59,462 - INFO - Loaded 3460 theses from database
2025-02-26 00:46:59,720 - INFO - TF-IDF embeddings prepared successfully
2025-02-26 00:47:13,822 - INFO - Making new LLM API call
2025-02-26 00:47:14,867 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 00:47:14,881 - INFO - Using cached LLM response
2025-02-26 00:47:14,882 - INFO - Using cached LLM response
2025-02-26 00:47:14,882 - INFO - Using cached LLM response
2025-02-26 00:47:14,883 - INFO - Using cached LLM response
2025-02-26 00:47:14,883 - INFO - Using cached LLM response
2025-02-26 00:47:14,883 - INFO - Using cached LLM response
2025-02-26 00:47:14,884 - INFO - Using cached LLM response
2025-02-26 00:47:14,884 - INFO - Using cached LLM response
2025-02-26 00:47:14,884 - INFO - Using cached LLM response
2025-02-26 00:47:14,884 - INFO - Using cached LLM response
2025-02-26 00:52:55,771 - INFO - Loaded 3460 theses from database
2025-02-26 00:52:56,031 - INFO - TF-IDF embeddings prepared successfully
2025-02-26 00:53:00,629 - INFO - Making new LLM API call
2025-02-26 00:53:01,510 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 00:53:01,525 - INFO - Using cached LLM response
2025-02-26 00:53:01,526 - INFO - Using cached LLM response
2025-02-26 00:53:01,526 - INFO - Using cached LLM response
2025-02-26 00:53:01,526 - INFO - Using cached LLM response
2025-02-26 00:53:01,527 - INFO - Using cached LLM response
2025-02-26 00:53:01,527 - INFO - Using cached LLM response
2025-02-26 00:53:01,527 - INFO - Using cached LLM response
2025-02-26 00:53:01,528 - INFO - Using cached LLM response
2025-02-26 00:53:01,528 - INFO - Using cached LLM response
2025-02-26 00:53:01,528 - INFO - Using cached LLM response
2025-02-26 01:06:07,168 - INFO - Loaded 3460 theses from database
2025-02-26 01:06:07,442 - INFO - TF-IDF embeddings prepared successfully
2025-02-26 01:06:13,135 - INFO - Making new LLM API call
2025-02-26 01:06:13,590 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 01:06:13,601 - INFO - Using cached LLM response
2025-02-26 01:06:13,602 - INFO - Using cached LLM response
2025-02-26 01:06:13,602 - INFO - Using cached LLM response
2025-02-26 01:06:13,602 - INFO - Using cached LLM response
2025-02-26 01:06:13,603 - INFO - Using cached LLM response
2025-02-26 01:06:13,603 - INFO - Using cached LLM response
2025-02-26 01:06:13,603 - INFO - Using cached LLM response
2025-02-26 01:06:13,603 - INFO - Using cached LLM response
2025-02-26 01:06:13,603 - INFO - Using cached LLM response
2025-02-26 01:06:13,604 - INFO - Using cached LLM response
2025-02-26 01:07:24,099 - INFO - Loaded 3460 theses from database
2025-02-26 01:07:24,358 - INFO - TF-IDF embeddings prepared successfully
2025-02-26 01:07:27,391 - INFO - Making new LLM API call
2025-02-26 01:07:28,156 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 01:07:28,171 - INFO - Using cached LLM response
2025-02-26 01:07:28,171 - INFO - Using cached LLM response
2025-02-26 01:07:28,171 - INFO - Using cached LLM response
2025-02-26 01:07:28,172 - INFO - Using cached LLM response
2025-02-26 01:07:28,172 - INFO - Using cached LLM response
2025-02-26 01:07:28,172 - INFO - Using cached LLM response
2025-02-26 01:07:28,173 - INFO - Using cached LLM response
2025-02-26 01:07:28,173 - INFO - Using cached LLM response
2025-02-26 01:07:28,173 - INFO - Using cached LLM response
2025-02-26 01:07:28,174 - INFO - Using cached LLM response
2025-02-26 01:07:54,696 - INFO - Using cached LLM response
2025-02-26 01:07:54,698 - INFO - Using cached LLM response
2025-02-26 01:07:54,698 - INFO - Using cached LLM response
2025-02-26 01:07:54,699 - INFO - Using cached LLM response
2025-02-26 01:07:54,699 - INFO - Using cached LLM response
2025-02-26 01:07:54,699 - INFO - Using cached LLM response
2025-02-26 01:07:54,700 - INFO - Using cached LLM response
2025-02-26 01:07:54,700 - INFO - Using cached LLM response
2025-02-26 01:07:54,701 - INFO - Using cached LLM response
2025-02-26 01:07:54,701 - INFO - Using cached LLM response
2025-02-26 01:08:09,783 - INFO - Making new LLM API call
2025-02-26 01:08:10,344 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 01:08:10,368 - INFO - Using cached LLM response
2025-02-26 01:08:10,368 - INFO - Using cached LLM response
2025-02-26 01:08:10,369 - INFO - Using cached LLM response
2025-02-26 01:08:10,369 - INFO - Using cached LLM response
2025-02-26 01:08:10,369 - INFO - Using cached LLM response
2025-02-26 01:08:34,605 - INFO - Making new LLM API call
2025-02-26 01:08:35,327 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 01:08:35,336 - INFO - Using cached LLM response
2025-02-26 01:08:35,337 - INFO - Using cached LLM response
2025-02-26 01:08:35,338 - INFO - Using cached LLM response
2025-02-26 01:08:35,339 - INFO - Using cached LLM response
2025-02-26 01:08:35,339 - INFO - Using cached LLM response
2025-02-26 01:08:35,339 - INFO - Using cached LLM response
2025-02-26 01:08:35,339 - INFO - Using cached LLM response
2025-02-26 01:08:35,340 - INFO - Using cached LLM response
2025-02-26 01:08:35,340 - INFO - Using cached LLM response
2025-02-26 01:08:35,340 - INFO - Using cached LLM response
2025-02-26 01:11:10,905 - INFO - Loaded 3460 theses from database
2025-02-26 01:11:11,162 - INFO - TF-IDF embeddings prepared successfully
2025-02-26 01:11:22,450 - INFO - Making new LLM API call
2025-02-26 01:11:23,370 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 01:11:23,386 - INFO - Using cached LLM response
2025-02-26 01:11:23,386 - INFO - Using cached LLM response
2025-02-26 01:11:23,387 - INFO - Using cached LLM response
2025-02-26 01:11:23,387 - INFO - Using cached LLM response
2025-02-26 01:11:23,387 - INFO - Using cached LLM response
2025-02-26 01:11:23,387 - INFO - Using cached LLM response
2025-02-26 01:11:23,388 - INFO - Using cached LLM response
2025-02-26 01:11:23,388 - INFO - Using cached LLM response
2025-02-26 01:11:23,389 - INFO - Using cached LLM response
2025-02-26 01:11:23,389 - INFO - Using cached LLM response
2025-02-26 01:13:13,483 - INFO - Loaded 3460 theses from database
2025-02-26 01:13:13,742 - INFO - TF-IDF embeddings prepared successfully
2025-02-26 01:13:22,575 - INFO - Making new LLM API call
2025-02-26 01:13:22,736 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-26 01:13:22,737 - ERROR - Error calling LLM API: Error code: 413 - {'error': {'message': 'Request too large for model `llama3-8b-8192` in organization `org_01j66m4gmpfahvz5snsjhe79cb` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6044, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-26 01:13:22,743 - INFO - Making new LLM API call
2025-02-26 01:13:23,071 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 01:13:23,095 - INFO - Making new LLM API call
2025-02-26 01:13:23,479 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 01:13:23,486 - INFO - Using cached LLM response
2025-02-26 01:13:23,487 - INFO - Making new LLM API call
2025-02-26 01:13:23,705 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 01:13:23,726 - INFO - Making new LLM API call
2025-02-26 01:13:24,093 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 01:13:24,102 - INFO - Making new LLM API call
2025-02-26 01:13:24,503 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 01:13:24,522 - INFO - Making new LLM API call
2025-02-26 01:13:24,811 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 01:13:24,818 - INFO - Making new LLM API call
2025-02-26 01:13:25,117 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 01:13:25,139 - INFO - Making new LLM API call
2025-02-26 01:13:25,395 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 01:13:25,402 - INFO - Making new LLM API call
2025-02-26 01:13:25,618 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 01:13:54,518 - INFO - Loaded 3460 theses from database
2025-02-26 01:13:54,778 - INFO - TF-IDF embeddings prepared successfully
2025-02-26 01:14:02,351 - INFO - Using cached LLM response
2025-02-26 01:14:02,352 - INFO - Using cached LLM response
2025-02-26 01:14:02,352 - INFO - Using cached LLM response
2025-02-26 01:14:02,352 - INFO - Using cached LLM response
2025-02-26 01:14:02,352 - INFO - Using cached LLM response
2025-02-26 01:14:02,352 - INFO - Using cached LLM response
2025-02-26 01:14:02,352 - INFO - Using cached LLM response
2025-02-26 01:14:02,353 - INFO - Using cached LLM response
2025-02-26 01:14:02,353 - INFO - Using cached LLM response
2025-02-26 01:14:02,353 - INFO - Using cached LLM response
2025-02-26 01:14:02,353 - INFO - Using cached LLM response
2025-02-26 01:14:35,268 - INFO - Loaded 3460 theses from database
2025-02-26 01:14:35,527 - INFO - TF-IDF embeddings prepared successfully
2025-02-26 01:14:38,822 - INFO - Making new LLM API call
2025-02-26 01:14:39,209 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 01:14:39,222 - INFO - Using cached LLM response
2025-02-26 01:14:48,705 - INFO - Making new LLM API call
2025-02-26 01:14:49,049 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 01:14:49,081 - INFO - Using cached LLM response
2025-02-26 01:14:49,081 - INFO - Using cached LLM response
2025-02-26 01:14:49,081 - INFO - Using cached LLM response
2025-02-26 01:14:49,082 - INFO - Using cached LLM response
2025-02-26 01:14:49,082 - INFO - Using cached LLM response
2025-02-26 01:15:00,652 - INFO - Making new LLM API call
2025-02-26 01:15:01,391 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 01:15:01,399 - INFO - Using cached LLM response
2025-02-26 01:15:01,399 - INFO - Using cached LLM response
2025-02-26 01:15:01,399 - INFO - Using cached LLM response
2025-02-26 01:15:01,400 - INFO - Using cached LLM response
2025-02-26 01:15:01,400 - INFO - Using cached LLM response
2025-02-26 01:15:01,401 - INFO - Using cached LLM response
2025-02-26 01:15:01,401 - INFO - Using cached LLM response
2025-02-26 01:15:01,401 - INFO - Using cached LLM response
2025-02-26 01:15:01,401 - INFO - Using cached LLM response
2025-02-26 01:15:01,402 - INFO - Using cached LLM response
2025-02-26 01:16:17,085 - INFO - Loaded 3460 theses from database
2025-02-26 01:16:17,342 - INFO - TF-IDF embeddings prepared successfully
2025-02-26 01:16:22,960 - INFO - Making new LLM API call
2025-02-26 01:16:24,014 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 01:16:24,029 - INFO - Using cached LLM response
2025-02-26 01:16:24,029 - INFO - Using cached LLM response
2025-02-26 01:16:24,030 - INFO - Using cached LLM response
2025-02-26 01:16:24,030 - INFO - Using cached LLM response
2025-02-26 01:16:24,031 - INFO - Using cached LLM response
2025-02-26 01:16:24,031 - INFO - Using cached LLM response
2025-02-26 01:16:24,031 - INFO - Using cached LLM response
2025-02-26 01:16:24,031 - INFO - Using cached LLM response
2025-02-26 01:16:24,032 - INFO - Using cached LLM response
2025-02-26 01:16:24,032 - INFO - Using cached LLM response
2025-02-26 01:17:08,753 - INFO - Making new LLM API call
2025-02-26 01:17:09,464 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 01:17:09,493 - INFO - Using cached LLM response
2025-02-26 01:17:09,494 - INFO - Using cached LLM response
2025-02-26 01:17:09,494 - INFO - Using cached LLM response
2025-02-26 01:17:09,494 - INFO - Using cached LLM response
2025-02-26 01:17:09,495 - INFO - Using cached LLM response
2025-02-26 01:17:09,495 - INFO - Using cached LLM response
2025-02-26 01:17:09,496 - INFO - Using cached LLM response
2025-02-26 01:17:09,496 - INFO - Using cached LLM response
2025-02-26 01:17:09,497 - INFO - Using cached LLM response
2025-02-26 01:17:09,497 - INFO - Using cached LLM response
2025-02-26 09:29:09,670 - INFO - Loaded 3460 theses from database
2025-02-26 09:29:09,929 - INFO - TF-IDF embeddings prepared successfully
2025-02-26 09:29:34,692 - INFO - Making new LLM API call
2025-02-26 09:29:36,001 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 09:29:36,016 - INFO - Using cached LLM response
2025-02-26 09:29:36,016 - INFO - Using cached LLM response
2025-02-26 09:29:36,017 - INFO - Using cached LLM response
2025-02-26 09:29:36,017 - INFO - Using cached LLM response
2025-02-26 09:29:36,017 - INFO - Using cached LLM response
2025-02-26 09:29:36,017 - INFO - Using cached LLM response
2025-02-26 09:29:36,018 - INFO - Using cached LLM response
2025-02-26 09:29:36,018 - INFO - Using cached LLM response
2025-02-26 09:29:36,018 - INFO - Using cached LLM response
2025-02-26 09:29:36,019 - INFO - Using cached LLM response
2025-02-26 09:33:14,288 - INFO - Loaded 3460 theses from database
2025-02-26 09:33:14,547 - INFO - TF-IDF embeddings prepared successfully
2025-02-26 09:33:36,035 - INFO - Making new LLM API call
2025-02-26 09:33:37,273 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 09:33:37,288 - INFO - Making new LLM API call
2025-02-26 09:33:37,781 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 09:33:37,798 - INFO - Making new LLM API call
2025-02-26 09:33:38,087 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 09:33:38,095 - INFO - Making new LLM API call
2025-02-26 09:33:38,327 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 09:33:38,348 - INFO - Making new LLM API call
2025-02-26 09:33:38,806 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 09:33:38,814 - INFO - Making new LLM API call
2025-02-26 09:33:39,317 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 09:33:39,337 - INFO - Making new LLM API call
2025-02-26 09:33:39,622 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 09:33:39,632 - INFO - Making new LLM API call
2025-02-26 09:33:39,876 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 09:33:39,896 - INFO - Making new LLM API call
2025-02-26 09:33:39,953 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-02-26 09:33:39,954 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds
2025-02-26 09:33:42,492 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 09:33:42,500 - INFO - Making new LLM API call
2025-02-26 09:33:42,559 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-02-26 09:33:42,560 - INFO - Retrying request to /openai/v1/chat/completions in 3.000000 seconds
2025-02-26 09:33:45,831 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 09:33:45,850 - INFO - Making new LLM API call
2025-02-26 09:33:45,924 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-02-26 09:33:45,924 - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds
2025-02-26 09:33:48,225 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-26 09:34:44,778 - INFO - Using cached LLM response
2025-02-26 09:34:44,780 - INFO - Using cached LLM response
2025-02-26 09:34:44,780 - INFO - Using cached LLM response
2025-02-26 09:34:44,780 - INFO - Using cached LLM response
2025-02-26 09:34:44,781 - INFO - Using cached LLM response
2025-02-26 09:34:44,781 - INFO - Using cached LLM response
2025-02-26 09:34:44,781 - INFO - Using cached LLM response
2025-02-26 09:34:44,781 - INFO - Using cached LLM response
2025-02-26 09:34:44,781 - INFO - Using cached LLM response
2025-02-26 09:34:44,781 - INFO - Using cached LLM response
2025-02-26 09:34:44,781 - INFO - Using cached LLM response
